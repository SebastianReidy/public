While working on a new feature for your team's product, your colleague is
required to write a function that takes a list of events and sorts them by
their timestamp. Using their algorithm course knowledge, they remind you
that [merge sort](https://en.wikipedia.org/wiki/Merge_sort)'s complexity
is `O(n log n)`, which is better than the `O(n^2)` worst-case complexity
of [quick sort](https://en.wikipedia.org/wiki/Quicksort), and that they will
therefore use the former to solve their task. What do you think of your
colleague's approach?

- I remind him that the landau notation makes a statement for the asymptotical case of very large Ns. Generally, there
can be very large constants hidden in the O(). They could make quicksort faster than merge-sort in this particular case. 
I suggest to implement both or even more sorting algorithms and benchmark them on a valid set of inputs(one that 
resembles real life). Choose the algo that actually has the lowest latency for this particular problem.  

**SOLUTION: Infact, common implementations use merge sort for longer inputs and quicksort for smaller inputs because 
the latter is faster in those cases because of the smaller overhead. Possibly, he should use standard library implementations 
that account for platform / language specific optimizations.**
